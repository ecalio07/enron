{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Fraud Detection Machine Learning on Enron Enteprise Dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Abstract\n",
    "\n",
    "The purpose of this project is to provide a reproducible paper regarding studies on how well Naive Bayes, SVM, and Decision Tree Machine Learning Algorithms can indentify emails by their authors using a pre-processed list of email texts and the corresponding authors based on the text dataset of the famous fraud scandal of the american bankrupt Enron Corporation. We will also study ways to work with parameters to improve accuracy and performance.\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Enron Corporation was an American energy, commodities, and services company based in Houston, Texas. It was founded in 1985 as the result of a merger between Houston Natural Gas and InterNorth. \n",
    "\n",
    "In the 1990s, the company became a leading energy marketer of natural gas, crude oil, electricity and liquids in North America, Europe and the rest of the world. It employed approximately 20,000 staff and was one of the world's major electricity, natural gas, communications and pulp and paper companies, with claimed revenues of nearly $101 billion during 2000. It was named as America's Most Innovative Company. \n",
    "\n",
    "However, the CEO Jeffrey Skilling had a way of hiding the financial losses of the trading business and other operations of the company, it was called ** mark-to-market accounting **. This is a technique used when trading securities where you measure the value of a security based on its current market value, instead of its book value. This can work well for securities, but it can be disastrous for other businesses.\n",
    "\n",
    "In Enron's case, the company would build an asset, such as a power plant, and immediately claim the projected profit on its books, even though it hadn't made any money from it. If the revenue from the power plant were less than the projected amount, instead of taking the loss, the company would then transfer these assets to an off-the-books corporation, where the loss would go unreported. This type of accounting enabled Enron to write off losses without hurting the company's bottom line.\n",
    "\n",
    "At the end of 2001, the company's corruption emmerged as a great fraud scandal and soon after this the company filed for bankruptcy.  After that, many rules and regulations were changed to be able to audit and prenvent cases like this one. \n",
    "\n",
    "This scandal brought into question the accounting practices and activities of many corporations in the United States and many rules and regulations were changed to be able to audit and prenvent cases like this one.  \n",
    "\n",
    "Many wonder how such a powerful business disintegrated so quickly and how it managed to fool the regulators for so long.\n",
    "\n",
    "Due to this interesting case, the company's financial and emails data became public for studies purpose. And this case became a point of interest for machine learning analysis because it could assist in finding solutions on how to prevent similar situations to happen. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Source\n",
    "\n",
    "The original [dataset](https://www.cs.cmu.edu/~./enron/) contains financial data and text features extracted from emails comprised of 146 users with 21 features each. For this experiment, two other datasets (/data/email_authors.pkl, /data/word_data.pk) will be used as the source data: **word_data.pk** corresponds to the **features**, and **email_authors.pkl** to the **labels**.\n",
    "\n",
    "They were created  by Katie Malone for Udacity machine learning training course. They represent 8.000 emails per user, belonging to two users: Chris and Sara. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related Works\n",
    "\n",
    "Contents and instructions used for this paper where based on the \"Udacity - Introduction to Machine Leaning course\", and were adaped according to the goals explained here.\n",
    "\n",
    "https://github.com/mdegis/machine-learning <br>\n",
    "https://github.com/baumanab/udacity_intro_machinelearning_project<br>\n",
    "https://github.com/skl92/machine-learning-enron-email-analysis<br>\n",
    "https://github.com/dshgna/ud120-projects<br>\n",
    "\n",
    "This is being used for educational pourposes only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments Workflow\n",
    "\n",
    "In real world, there has been some fraud cases where one author publishes its book in the name of another. For this experiments sake, instead of text books, we will have have a set of emails text, half of which were written by one person and the other half by another person. \n",
    "\n",
    "As such, three different classifiers will be trainned to predict autors emails by their word contents and styles. It will be performed arguments configuration according to each classifier below so as to reach best time performance and accurance, as well as comparisons of results.\n",
    "\n",
    "For each experiment, there will be a cell (below descriptions) to reproduce results and the option to access the code and have the chance to change values for other results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   * SVM\n",
    "\n",
    "Support Vector Machine works very well in complicated domains with clear margin of separation but it doesn't perform well in very large datasets, for it can become slow and prone to overfitting. As for tunning we can conclude that best accuracy were achieved with  parameters RBF kernel, C=10000, and full dataset. As for performance, there will always be a tradeoff with accuracy reducing the dataset to make the code faster.\n",
    "\n",
    "It separate two classes creating a line separator(decision boundary), handling well margims and outliers. \n",
    "\n",
    "For this experiment we will work on changing values for paremeter the parameters **C**, **kernel** and **gamma**. when initiating SVC function. It can be a simple choice with few parameter (ex 1), multiple paramenter (ex 2) or no parameters at all.\n",
    "\n",
    "* **ex 1** <br>\n",
    "linear_kernel_svm = svm.SVC(kernel='rbf', C=10000.)\n",
    "\n",
    "* **ex 2** <br>\n",
    "linear_kernel_svm = svm.SVC(**C=1.0**, **kernel**='rbf', degree=3, **gamma='auto'**, coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape=None, random_state=None)[source]\n",
    "\n",
    "In machine learning we should avoid OVERFITTING. Because of that, we wil tune the parameters below since all of them affect overfitting and results like accuracy, performance.\n",
    "\n",
    "**C:** controls the tradeoff between smooth decision boundary and classification training points correctly. In theory, a large value of C means that you will get more training points correctly.\n",
    "\n",
    "**gamma:** defines how far a the influence of a single training example reaches. If gamma has a low value, every point has a far reach. If gamma has a high value, each training example has a close reach. High value might make the decision boundary less linear, for it will be closer to training points.\n",
    "\n",
    "**kernel** parameter can be ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’ or a callable. If none is given, ‘rbf’ will be used.\n",
    "\n",
    "For more information on SVM parameters, please click [here](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM Experiment  - Focus on Kernel Parameter: RBF vs Linear values\n",
    "\n",
    "Kernel values can be ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’ or a callable. The default one is rbf.\n",
    "\n",
    "Accurace is not good enogh by using **rbf(default value)** kernel alone. We can reach a slight improvement in accurace by change the value to **\"linear\"**.\n",
    "\n",
    "[code access](../dev/svm_kernel_parameter.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run ../dev/svm_kernel_parameter.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM Experiment  - Focus on Gamma Parameter.\n",
    "\n",
    "In this testing we will make changes to the gamma parameter value and addressing high and low values.\n",
    "This condition can be tested with rbf kernel and linear.\n",
    "\n",
    "Gamma played a major difference when working with RBF. Thus, when we have a high gamma value(say 10000), the accuracy score is very low (0.5). However, when gamma is low, accuracy is much better.\n",
    "\n",
    "On the other hand, gamma parameter made no difference in accuracy when working with linear kernel. Results didn't change.\n",
    "\n",
    "[code access](../dev/svm_gama_parameter.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%run ../dev/svm_gama_parameter.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM Experiment  - Focus on C Parameter.\n",
    "\n",
    "Trying several values of C (say, 10.0, 100., 1000., and 10000.) and recording results, we can notice that the higher is the value of C, the better is accuracy until it reaches a limit where there is no change in results, and therefore, no reason to continue increasing.\n",
    "\n",
    "The interesting part o this experiment is that C parameters affect results when working with linear and rbf. Best results where met when working with linear kernel and high C values.\n",
    "\n",
    "\n",
    "[code access](../dev/svm_C_parameter.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%run ../dev/svm_C_parameter.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### SVM Experiment  - Focus on Accuracy vs Performance\n",
    "\n",
    "In this testing we will improve the accuracy at the cost of performance. However, it is possible to have a middle term result, depending on reduction size of the data set. Accuracy is much better with full dataset.\n",
    "\n",
    "\n",
    "[code access](../dev/svm_accuracy_vs_performance.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run ../dev/svm_accuracy_vs_performance.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  * GaussianNB (Naive Bayes)\n",
    "\n",
    "Naive Bayes is a supervised classification algorithm used substancially in learning from documents (text learning). Each word is considered a feature and user names are considered the labes. It is called Naive because it ignores the words order.\n",
    "\n",
    "It is really easy to implement and efficient. The relative simplicity of the algorithm and the independent features assumption of Naive Bayes make it a strong performer for classifying texts. It is good when working with a lot of noise of the data. On the other hand, it can break for some phrases for considering the words individually.\n",
    "\n",
    "The classifier uses Posterior Probability, giving the rank occurance provided text. In order words, it will be trained with frequent texts(features) used by Chris and Sarah(labels), and it will calculate the probabily and determine if each test email is from Chris or Sara.\n",
    "\n",
    "It is possible to work with parameters, just like SVM. But their changes made no difference in results.\n",
    "Using Naive Bayes, it was possible to reach a good accurace and it was simpler, for it was not necessary to play with parameter changes.\n",
    "\n",
    "For more information on GaussianNB parameters, please click [here](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html)\n",
    "\n",
    "[code access](../dev/bayes.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%run ../dev/bayes.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  * Decision Tree\n",
    "   Decision Trees are simple to understand and interpret, but do not tend to be as accurate as other approaches.\n",
    "   They also prone to overfitting.\n",
    "   \n",
    "   Main parameters covered in this experiment will be:\n",
    "   * min_samples_split: it controls how deep the tree will reach\n",
    "   * percentile: \n",
    "   \n",
    "   For more information on Decidion Tree parameters, please click [here](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tree Experiment  - Focus on min_samples_split  parameter.\n",
    "\n",
    "In this testing we will make changes to the min_samples_split parameter value and addressing high and low values.\n",
    "This condition can be tested with rbf kernel and linear.\n",
    "\n",
    "\n",
    "[code access](../dev/min_samples_split_parameter.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%run ../dev/min_samples_split_parameter.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tree Experiment  - Focus on percentile  parameter.\n",
    "\n",
    "By changing percentile parameter to a higher number, it was possible to achieve improvements in accuracy.\n",
    "\n",
    "\n",
    "[code access](../dev/tree_percentile_parameter.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%run ../dev/tree_percentile_parameter.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summury of Results\n",
    "\n",
    "These experiments explored ways to use algorithm and configuration options on how to find most suitable approaches for handling text identification using machine learning.\n",
    "\n",
    "#### Analysis with Reduced DataSet:\n",
    "When working with SVM with only kernel parameter, linear value is better. However, when we added the gamma parameter with linear option, accuracy was badly affected. C parameter with high or low value, made no difference in results with linear kernel value.\n",
    "\n",
    "Making a cross comparation among the options above, best linear result were met working with no other paramenters except kernel=linear. \n",
    "\n",
    "With regards to RBF, we rechead the highest SVM accuracy by combining Linear kernel=rbf and Gamma parameter with Low value (0.889078498294).\n",
    "\n",
    "The best result with reduced dataset was reached with the combination kernel='linear', C=10000 = (0.892491467577)\n",
    "\n",
    "Decison tree present bad accuracy with reduced dataset, around 0.776450511945.\n",
    "\n",
    "#### Analysis with Full DataSet:\n",
    "\n",
    "\n",
    "Altough SVM full dataset reached highest accuracy of all (0.990898748578), it was the slowest algorithm.\n",
    "\n",
    "Decision Tree presented great accuracy and performance with larger datasets.\n",
    "\n",
    "When we tested naive bayes, accuracy with full dataset was (0.973265073948). It was the second best result of all experiments but performance was much better than SVM.\n",
    "\n",
    "**Conclusion:**\n",
    "Based on the experiments here presented, I would exclude SVM due to complexity and slow performance. Even though accuracy was the best.\n",
    "\n",
    "Results suggests that Decision Tree and GaussianNB (Naive Bayes) would be great options to work with larger datasets and meet good accuarcy and performance. \n",
    "\n",
    "GaussianNB presented the best perfomance of all and good accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    " Main: https://classroom.udacity.com/courses/ud120"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
